{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wi9N6Eax7kY",
        "outputId": "28b78eba-749f-43d7-c9af-6844df52bb54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"cuda available? \" + str(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OceGc3QuyBBv",
        "outputId": "87c93453-8f53-436f-ed3c-f2dee3ef0503"
      },
      "outputs": [],
      "source": [
        "# Define root directory from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "root_dir = \"/content/drive/My Drive/ceilometer_dataset1.1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fkdWtoayFsX"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((300,300)),\n",
        "    transforms.RandomResizedCrop(300),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Augmentation on test images not needed\n",
        "transforms_test = torchvision.transforms.Compose([\n",
        "    transforms.Resize((300,300)),\n",
        "    transforms.RandomResizedCrop(300),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN2JRQ4TyoyP"
      },
      "source": [
        "Structure of dataset folders\n",
        "```\n",
        "ceilometer_dataset\n",
        "    ----> train\n",
        "          ---->true\n",
        "          ---->false\n",
        "    ----> test\n",
        "          ---->true\n",
        "          ---->false\n",
        "\n",
        "70% train, 30% test\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmw28KAnypZa"
      },
      "outputs": [],
      "source": [
        "# remember to modify placeholders!\n",
        "train_dir = root_dir + \"train/\"\n",
        "test_dir = root_dir +\"test/\"\n",
        "train_classa_dir = root_dir + \"train/true/\"\n",
        "train_classb_dir = root_dir + \"train/false/\"\n",
        "test_classa_dir = root_dir + \"test/true/\"\n",
        "test_classb_dir = root_dir + \"test/false/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO8Arbg3yrWy"
      },
      "outputs": [],
      "source": [
        "train_datasets = datasets.ImageFolder(train_dir, transforms_train)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transforms_test)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_datasets, [round(len(train_datasets)*0.70), round(len(train_datasets)*0.30)])\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=True, num_workers=2, drop_last=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9HBzL8ytNS",
        "outputId": "2372378c-903d-49c8-c093-32e1fea807a6"
      },
      "outputs": [],
      "source": [
        "print('Train dataset size:', len(train_dataset))\n",
        "print('Validation dataset size:', len(val_dataset))\n",
        "print('Test dataset size:', len(test_dataset))\n",
        "class_names = test_dataset.classes\n",
        "print('Class names:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "kbM7nNhzyvAo",
        "outputId": "879a4984-253d-4a2a-b996-401a4fef02ba"
      },
      "outputs": [],
      "source": [
        "# Random checking of train images\n",
        "im, label = train_dataset[100]\n",
        "print(im.shape)\n",
        "print(\"-------------\")\n",
        "print(label)\n",
        "plt.imshow(im.permute(1,2,0).cpu())\n",
        "print(\"Class: \", class_names[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYznwtMqyyhU"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.resnetmodel = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(1000, 512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnetmodel(x)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVyaFK_JzBdk",
        "outputId": "d14a1d51-56a0-45d5-d8de-6588ca5effca"
      },
      "outputs": [],
      "source": [
        "model = Model()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.8, weight_decay=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiups4eilhqp",
        "outputId": "45909551-ea08-4361-944c-9a62d7857549"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_SpDLXty8_E"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfsUKHvAzRLj"
      },
      "outputs": [],
      "source": [
        "# Initialize training history\n",
        "# Initialize history\n",
        "history_loss = {\"train\": [], \"val\": [],\"test\": []}\n",
        "history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_f1 = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_precision = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_recall = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "# Initialize best validation accuracy and test accuracy at best validation accuracy\n",
        "best_val_accuracy = 0\n",
        "best_test_accuracy = 0\n",
        "best_test_f1 = 0\n",
        "best_test_precision = 0\n",
        "best_test_recall = 0\n",
        "save_test_value = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWbsUK6VzRsp",
        "outputId": "2b0f6fd0-9e5b-473f-ebe6-db68f6a6b49e"
      },
      "outputs": [],
      "source": [
        "num_epochs = 60   #(set no of epochs)\n",
        "early_stopper = EarlyStopper(patience=3, min_delta=0.05)\n",
        "start_time = time.time() #(for showing time)\n",
        "for epoch in range(num_epochs): #(loop for every epoch)\n",
        "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
        "    \"\"\" Training Phase \"\"\"\n",
        "    model.train()    #(training model)\n",
        "    running_loss = 0   #(set loss 0)\n",
        "    running_corrects = 0\n",
        "    running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "    # load a batch data of images\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # forward inputs and get output\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # get loss value and update the network weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "        running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "        running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
        "\n",
        "    epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "    epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "    for i in range(2, len(running_labels_preds['labels'])):\n",
        "      epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "      epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "    epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "    epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "    epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "    history_loss[\"train\"].append(epoch_loss)\n",
        "    history_accuracy[\"train\"].append(epoch_acc)\n",
        "    history_f1[\"train\"].append(epoch_f1)\n",
        "    history_precision[\"train\"].append(epoch_precision)\n",
        "    history_recall[\"train\"].append(epoch_recall)\n",
        "\n",
        "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
        "\n",
        "    \"\"\" Validation Phase \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.\n",
        "        running_corrects = 0\n",
        "        running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(val_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "            running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "            running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "\n",
        "        epoch_loss = running_loss / len(val_dataset)\n",
        "        epoch_acc = running_corrects / len(val_dataset) * 100.\n",
        "\n",
        "        epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "        epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "        for i in range(2, len(running_labels_preds['labels'])):\n",
        "          epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "          epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "        epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "        epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "        epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "        history_loss[\"val\"].append(epoch_loss)\n",
        "        history_accuracy[\"val\"].append(epoch_acc)\n",
        "        history_f1[\"val\"].append(epoch_f1)\n",
        "        history_precision[\"val\"].append(epoch_precision)\n",
        "        history_recall[\"val\"].append(epoch_recall)\n",
        "\n",
        "        if epoch_acc > best_val_accuracy:\n",
        "          best_val_accuracy = epoch_acc\n",
        "          save_test_value = True\n",
        "        else:\n",
        "          save_test_value = False\n",
        "        print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))\n",
        "        # early stopping\n",
        "        if early_stopper.early_stop(epoch_loss):\n",
        "          break\n",
        "\n",
        "    \"\"\" Testing Phase \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "        running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "            running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "            running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "\n",
        "        epoch_loss = running_loss / len(test_dataset)\n",
        "        epoch_acc = running_corrects / len(test_dataset) * 100.\n",
        "\n",
        "        epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "        epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "        for i in range(2, len(running_labels_preds['labels'])):\n",
        "          epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "          epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "        epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "        epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "        epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "        history_loss[\"test\"].append(epoch_loss)\n",
        "        history_accuracy[\"test\"].append(epoch_acc)\n",
        "        history_f1[\"test\"].append(epoch_f1)\n",
        "        history_precision[\"test\"].append(epoch_precision)\n",
        "        history_recall[\"test\"].append(epoch_recall)\n",
        "\n",
        "        if save_test_value == True:\n",
        "          best_test_accuracy = epoch_acc\n",
        "          best_test_f1 = epoch_f1\n",
        "          best_test_precision = epoch_precision\n",
        "          best_test_recall = epoch_recall\n",
        "        print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc0huc1tzMCj",
        "outputId": "e9825b7a-50fe-4965-e667-0587931c5113"
      },
      "outputs": [],
      "source": [
        "nb_classes = 2\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(test_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "-Gf9sLmFzTxq",
        "outputId": "4aedae9f-216a-496c-e7fb-1f104ff78937"
      },
      "outputs": [],
      "source": [
        "# Plot loss history\n",
        "plt.title(\"Loss\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_loss[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "9180jpjczXsL",
        "outputId": "1ecd1a3a-cf39-4541-e44f-e15396c8aec1"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(history_accuracy[\"train\"])):\n",
        "  history_accuracy[\"train\"][i]=history_accuracy[\"train\"][i].cpu()\n",
        "for i in range(0, len(history_accuracy[\"val\"])):\n",
        "  history_accuracy[\"val\"][i]=history_accuracy[\"val\"][i].cpu()\n",
        "for i in range(0, len(history_accuracy[\"test\"])):\n",
        "  history_accuracy[\"test\"][i]=history_accuracy[\"test\"][i].cpu()\n",
        "\n",
        "# Plot accuracy history\n",
        "plt.title(\"Accuracy\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_accuracy[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ctoIz_nplhqr",
        "outputId": "e15a8d03-04c3-465f-f5d3-a96e1bb96024"
      },
      "outputs": [],
      "source": [
        "# Plot F1 history\n",
        "plt.title(\"F1 Score\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_f1[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "xVg_dsohlhqr",
        "outputId": "cc2d92a5-4ce0-471a-d004-770225e70f44"
      },
      "outputs": [],
      "source": [
        "# Plot precision history\n",
        "plt.title(\"Precision\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_precision[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1PO1m7yglhqr",
        "outputId": "3713cd73-2bac-47e8-e9ac-01eb4570dcff"
      },
      "outputs": [],
      "source": [
        "# Plot recall history\n",
        "plt.title(\"Recall\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_recall[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQfxUfGLzaDL",
        "outputId": "2e9be534-69dc-47be-99a6-8b38ec4f2325"
      },
      "outputs": [],
      "source": [
        "print(best_test_accuracy)\n",
        "print(best_test_f1)\n",
        "print(best_test_precision)\n",
        "print(best_test_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHL1GQs8zcF9"
      },
      "outputs": [],
      "source": [
        "save_path = 'efficientnet_SGD_6.pth'\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
