{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8CbOFRyaDi8",
        "outputId": "31e1cc6e-a99a-49a0-eb25-a048d6243dc7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
        "print(\"cuda available? \" + str(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3BDeppySPk",
        "outputId": "3ca2ea44-69fe-429d-86dc-032d362d7305"
      },
      "outputs": [],
      "source": [
        "# Define root directory from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "root_dir = \"/content/drive/My Drive/ceilometer_dataset1.1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OoP0iZoaPep"
      },
      "outputs": [],
      "source": [
        "# Image resize must be (224 * 224) because Resnet accepts input image size of (224 * 224)\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   #must same as here\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(), # data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
        "])\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # must same as here\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyjfRrrati6"
      },
      "source": [
        "Structure of dataset folders\n",
        "```\n",
        "ceilometer_dataset\n",
        "    ----> train\n",
        "          ---->true\n",
        "          ---->false\n",
        "    ----> test\n",
        "          ---->true\n",
        "          ---->false\n",
        "\n",
        "70% train, 30% test\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RZNSGDqaf31"
      },
      "outputs": [],
      "source": [
        "# remember to modify placeholders!\n",
        "train_dir = root_dir + \"train/\"\n",
        "test_dir = root_dir +\"test/\"\n",
        "train_classa_dir = root_dir + \"train/true/\"\n",
        "train_classb_dir = root_dir + \"train/false/\"\n",
        "test_classa_dir = root_dir + \"test/true/\"\n",
        "test_classb_dir = root_dir + \"test/false/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwWkBXvAaq5L"
      },
      "outputs": [],
      "source": [
        "train_datasets = datasets.ImageFolder(train_dir, transforms_train)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transforms_test)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_datasets, [round(len(train_datasets)*0.70), round(len(train_datasets)*0.30)])\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=12, shuffle=True, num_workers=2, drop_last=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMZ-pEIob02d",
        "outputId": "5636d4b0-fca2-499d-b58e-5cf52233c9d5"
      },
      "outputs": [],
      "source": [
        "print('Train dataset size:', len(train_dataset))\n",
        "print('Validation dataset size:', len(val_dataset))\n",
        "print('Test dataset size:', len(test_dataset))\n",
        "class_names = test_dataset.classes\n",
        "print('Class names:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "5h67jDpMb4EQ",
        "outputId": "b17f41a4-8da7-492a-cfd7-ab9a1558d1bc"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 60\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "def imshow(input, title):\n",
        "    # torch.Tensor => numpy\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # undo image normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # display images\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "# load a batch of train image\n",
        "iterator = iter(train_dataloader)\n",
        "# visualize a batch of train image\n",
        "inputs, classes = next(iterator)\n",
        "out = torchvision.utils.make_grid(inputs[:4])\n",
        "imshow(out, title=[class_names[x] for x in classes[:4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW2zzfL_cGzO"
      },
      "outputs": [],
      "source": [
        "model = models.resnet50(weights='IMAGENET1K_V2')   #load resnet50 model\n",
        "num_features = model.fc.in_features     #extract fc layers features\n",
        "model.fc = nn.Linear(num_features, 2) #(num_of_class == 2)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utGrC5SdBllO",
        "outputId": "7bb42e6d-b4f3-4a6b-8fd1-851392a9d30d"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khjjsFSDCQ2i"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njs6vC6tTC2i"
      },
      "outputs": [],
      "source": [
        "# Initialize training history\n",
        "# Initialize history\n",
        "history_loss = {\"train\": [], \"val\": [],\"test\": []}\n",
        "history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_f1 = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_precision = {\"train\": [], \"val\": [], \"test\": []}\n",
        "history_recall = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "# Initialize best validation accuracy and test accuracy at best validation accuracy\n",
        "best_val_accuracy = 0\n",
        "best_test_accuracy = 0\n",
        "best_test_f1 = 0\n",
        "best_test_precision = 0\n",
        "best_test_recall = 0\n",
        "save_test_value = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A33_ZTc8cNB2",
        "outputId": "bc9f271e-60f3-495d-a720-3ed9723771d8"
      },
      "outputs": [],
      "source": [
        "num_epochs = 60   #(set no of epochs)\n",
        "early_stopper = EarlyStopper(patience=3, min_delta=0.05)\n",
        "start_time = time.time() #(for showing time)\n",
        "for epoch in range(num_epochs): #(loop for every epoch)\n",
        "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
        "    \"\"\" Training Phase \"\"\"\n",
        "    model.train()    #(training model)\n",
        "    running_loss = 0   #(set loss 0)\n",
        "    running_corrects = 0\n",
        "    running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "    # load a batch data of images\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # forward inputs and get output\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # get loss value and update the network weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "        running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "        running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
        "\n",
        "    epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "    epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "    for i in range(2, len(running_labels_preds['labels'])):\n",
        "      epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "      epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "    epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "    epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "    epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "    history_loss[\"train\"].append(epoch_loss)\n",
        "    history_accuracy[\"train\"].append(epoch_acc)\n",
        "    history_f1[\"train\"].append(epoch_f1)\n",
        "    history_precision[\"train\"].append(epoch_precision)\n",
        "    history_recall[\"train\"].append(epoch_recall)\n",
        "\n",
        "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
        "\n",
        "    \"\"\" Validation Phase \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.\n",
        "        running_corrects = 0\n",
        "        running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "            running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "            running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "\n",
        "        epoch_loss = running_loss / len(val_dataset)\n",
        "        epoch_acc = running_corrects / len(val_dataset) * 100.\n",
        "\n",
        "        epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "        epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "        for i in range(2, len(running_labels_preds['labels'])):\n",
        "          epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "          epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "        epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "        epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "        epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "        history_loss[\"val\"].append(epoch_loss)\n",
        "        history_accuracy[\"val\"].append(epoch_acc)\n",
        "        history_f1[\"val\"].append(epoch_f1)\n",
        "        history_precision[\"val\"].append(epoch_precision)\n",
        "        history_recall[\"val\"].append(epoch_recall)\n",
        "\n",
        "        if epoch_acc > best_val_accuracy:\n",
        "          best_val_accuracy = epoch_acc\n",
        "          save_test_value = True\n",
        "        else:\n",
        "          save_test_value = False\n",
        "        print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))\n",
        "        # early stopping\n",
        "        if early_stopper.early_stop(epoch_loss):\n",
        "          break\n",
        "\n",
        "\n",
        "    \"\"\" Testing Phase \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.\n",
        "        running_corrects = 0\n",
        "        running_labels_preds = {\"labels\": [], \"preds\": []}\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # store labels and preds and calculate f1, precision and recall at the end of the epoch\n",
        "            running_labels_preds[\"labels\"].append(labels.to('cpu').data.numpy())\n",
        "            running_labels_preds[\"preds\"].append(preds.to('cpu'))\n",
        "\n",
        "        epoch_loss = running_loss / len(test_dataset)\n",
        "        epoch_acc = running_corrects / len(test_dataset) * 100.\n",
        "        epoch_labels = np.concatenate((running_labels_preds['labels'][0],running_labels_preds['labels'][1]), axis=None)\n",
        "        epoch_preds = np.concatenate((running_labels_preds['preds'][0],running_labels_preds['preds'][1]), axis=None)\n",
        "        for i in range(2, len(running_labels_preds['labels'])):\n",
        "          epoch_labels = np.concatenate((epoch_labels,running_labels_preds['labels'][i]), axis=None)\n",
        "          epoch_preds = np.concatenate((epoch_preds,running_labels_preds['preds'][i]), axis=None)\n",
        "\n",
        "        epoch_f1 = f1_score(epoch_labels, epoch_preds)\n",
        "        epoch_precision = precision_score(epoch_labels, epoch_preds)\n",
        "        epoch_recall = recall_score(epoch_labels, epoch_preds)\n",
        "\n",
        "        history_loss[\"test\"].append(epoch_loss)\n",
        "        history_accuracy[\"test\"].append(epoch_acc)\n",
        "        history_f1[\"test\"].append(epoch_f1)\n",
        "        history_precision[\"test\"].append(epoch_precision)\n",
        "        history_recall[\"test\"].append(epoch_recall)\n",
        "\n",
        "        if save_test_value == True:\n",
        "          best_test_accuracy = epoch_acc\n",
        "          best_test_f1 = epoch_f1\n",
        "          best_test_precision = epoch_precision\n",
        "          best_test_recall = epoch_recall\n",
        "        print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWmo6HbKFZXW",
        "outputId": "12003b4d-35d3-427a-a283-bc07fbbb7f12"
      },
      "outputs": [],
      "source": [
        "nb_classes = 2\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(test_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "cLq7KC3fU-zK",
        "outputId": "8a90fa46-afff-4989-e83f-24b921af3f6c"
      },
      "outputs": [],
      "source": [
        "# Plot loss history\n",
        "plt.title(\"Loss\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_loss[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "IFEzZSPUU_vp",
        "outputId": "1e763681-b299-4547-b1ce-6d027cc283b6"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(history_accuracy[\"train\"])):\n",
        "  history_accuracy[\"train\"][i]=history_accuracy[\"train\"][i].cpu()\n",
        "for i in range(0, len(history_accuracy[\"val\"])):\n",
        "  history_accuracy[\"val\"][i]=history_accuracy[\"val\"][i].cpu()\n",
        "for i in range(0, len(history_accuracy[\"test\"])):\n",
        "  history_accuracy[\"test\"][i]=history_accuracy[\"test\"][i].cpu()\n",
        "\n",
        "# Plot accuracy history\n",
        "plt.title(\"Accuracy\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_accuracy[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "rJFb9GRZtgpk",
        "outputId": "eec0cb16-8c07-47aa-cd8a-2e8454c7bf5c"
      },
      "outputs": [],
      "source": [
        "# Plot F1 history\n",
        "plt.title(\"F1 Score\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_f1[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "KwTxPNqWtgpk",
        "outputId": "3c8baf9e-db97-4e10-c7d0-5763b1d98a84"
      },
      "outputs": [],
      "source": [
        "# Plot precision history\n",
        "plt.title(\"Precision\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_precision[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "1NxwEOeutgpk",
        "outputId": "e0cc4d3b-bacd-4bed-fea1-2ee7a5f751cd"
      },
      "outputs": [],
      "source": [
        "# Plot recall history\n",
        "plt.title(\"Recall\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "  plt.plot(history_recall[split], label=split)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYLsdj_0VG3n",
        "outputId": "2a271234-e36e-4d3e-ca18-82e71a6d86fd"
      },
      "outputs": [],
      "source": [
        "print(best_test_accuracy)\n",
        "print(best_test_f1)\n",
        "print(best_test_precision)\n",
        "print(best_test_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rPLkpDrcTfY"
      },
      "outputs": [],
      "source": [
        "save_path = 'resnet50_Adam_1.pth'\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
